{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5429a37e-f46e-4311-ba9e-9ae3965e2a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-index                       0.9.48\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | findstr \"llama-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a07502e5-3448-4f93-a8f6-892b8290b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0687c32-8a39-4159-959f-bb4fdb54b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.evaluation import generate_question_context_pairs\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.evaluation import generate_question_context_pairs\n",
    "from llama_index.evaluation import RetrieverEvaluator\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91d9c874-3e00-42bc-b921-9f2361d313ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set Your OpenAI API Key\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5b1da92-9711-4f83-8471-23342e1c2a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents successfully!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "os.makedirs(\"data/paul_graham\", exist_ok=True)\n",
    "\n",
    "# Download the text file\n",
    "url = \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\"\n",
    "save_path = \"data/paul_graham/paul_graham_essay.txt\"\n",
    "urllib.request.urlretrieve(url, save_path)\n",
    "\n",
    "# Load data\n",
    "from llama_index import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"data/paul_graham\").load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94564d76-97a7-4d50-bfb2-fc986d3ac060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The author, growing up, worked on writing short stories and programming on an IBM 1401 computer in 9th grade using an early version of Fortran.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define an LLM\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)  \n",
    "# Build index with a chunk_size of 512\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "\n",
    "## Build a QueryEngine and start querying.\n",
    "\n",
    "query_engine = vector_index.as_query_engine()\n",
    "response_vector = query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "\n",
    "## Check response.\n",
    "response_vector.response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1edb0121-661b-4c72-811c-3f4e6f98782b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It felt like I was doing life right. I remember that because I was slightly dismayed at how novel it felt. The good news is that I had more moments like this over the next few years.\\n\\nIn the summer of 2016 we moved to England. We wanted our kids to see what it was like living in another country, and since I was a British citizen by birth, that seemed the obvious choice. We only meant to stay for a year, but we liked it so much that we still live there. So most of Bel was written in England.\\n\\nIn the fall of 2019, Bel was finally finished. Like McCarthy's original Lisp, it's a spec rather than an implementation, although like McCarthy's Lisp it's a spec expressed as code.\\n\\nNow that I could write essays again, I wrote a bunch about topics I'd had stacked up. I kept writing essays through 2020, but I also started to think about other things I could work on. How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives. So I wrote a more detailed version for others to read, and this is the last sentence of it.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotes\\n\\n[1] My experience skipped a step in the evolution of computers: time-sharing machines with interactive OSes. I went straight from batch processing to microcomputers, which made microcomputers seem all the more exciting.\\n\\n[2] Italian words for abstract concepts can nearly always be predicted from their English cognates (except for occasional traps like polluzione). It's the everyday words that differ. So if you string together a lot of abstract concepts with a few simple verbs, you can make a little Italian go a long way.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# By default it retrieves two similar nodes/ chunks. You can modify that in vector_index.as_query_engine(similarity_top_k=k).\n",
    "# First retrieved node\n",
    "response_vector.source_nodes[0].get_text()\n",
    "\n",
    "# Second retrieved node\n",
    "response_vector.source_nodes[1].get_text()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f5fa4ec2-638b-4bb1-899e-ba7aa9b1e4b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 59/59 [00:59<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Evaluation\n",
    "\n",
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes,\n",
    "    llm=llm,\n",
    "    num_questions_per_chunk=2\n",
    ")\n",
    "\n",
    "## Retrieval Evaluation:\n",
    "# We use Hit Rate and MRR metrics to evaluate our Retriever.\n",
    "#\n",
    "# Hit Rate:\n",
    "#\n",
    "# Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses.\n",
    "#\n",
    "# Mean Reciprocal Rank (MRR):\n",
    "#\n",
    "# For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it’s second, the reciprocal rank is 1/2, and so on.\n",
    "\n",
    "retriever = vector_index.as_retriever(similarity_top_k=2)\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    ")\n",
    "# Evaluate\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9201d27-610f-4236-bae9-9ec56840d1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>Hit Rate</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OpenAI Embedding Retriever</td>\n",
       "      <td>0.79661</td>\n",
       "      <td>0.673729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Retriever Name  Hit Rate       MRR\n",
       "0  OpenAI Embedding Retriever   0.79661  0.673729"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame(\n",
    "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
    "    )\n",
    "\n",
    "    return metric_df\n",
    "\n",
    "display_results(\"OpenAI Embedding Retriever\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd72d8b-378f-4181-b38f-303091d72943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
